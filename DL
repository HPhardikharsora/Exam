Q6 From pretrained network MobileNetV2, what is the name of layer at index 103?
from keras.applications import MobileNetV2
model = MobileNetV2()
model.layers[103]
Q2. Using the self-attention mechanism (without trainable weights), what is the new context vector for the input embedding at index = 1?
import numpy as np
X = np.array([
    [-5.2, 3.1, 7.4],
    [2.3, -6.5, 1.8],
    [4.1, -2.7, 0.9],
    [-1.6, 8.3, -4.5],
    [0.5, -1.4, 3.2]
])
embedding_index_1 = X[1]
similarity_scores = np.dot(X, embedding_index_1)
exp_scores = np.exp(similarity_scores - np.max(similarity_scores))  # For numerical stability
attention_weights = exp_scores / np.sum(exp_scores)
context_vector = np.dot(attention_weights, X)
context_vector

 
model = VGG16(
    include_top=False,
    input_shape=(188, 188, 3)
    )
model.layers

for layers in model.layers[:12]:
    layers.trainable = False

for layer in model.layers:
    print(f"Is layer {layer.name} trainable? {layer.trainable}")

from keras import layers
import keras

inputs = layers.Input(shape=(188, 188, 3))
x = layers.Rescaling(1./255) (inputs) # Rescale the pixel values to the range [0, 1]
x = keras.applications.vgg16.preprocess_input(x)
x = model(x)
x = layers.Flatten()(x)
x = layers.Dense(512, activation="relu")(x)
x = layers.Dense(512, activation="relu")(x)
outputs = layers.Dense(5, activation="softmax")(x)

full_model = keras.Model(inputs, outputs)

full_model.summary()


Q4. Using a Keras embedding layer, what would be the token embedding for the input at index 3 of input _ids = np.array([135, 681, 832, 116, 532, 284, 462, 2771). The vocabulary size is 962 and the NumPy; the seed is 124. import random as python _random python_random.seed(seed) 

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
import random as python_random
python_random.seed(124)
# Parameters
vocab_size = 962
emb_dim = 4
input_ids = np.array([135, 681, 832, 116, 532, 284, 462, 277])

# Create an Embedding layer
emb_layer = layers.Embedding(input_dim=vocab_size, output_dim=emb_dim)

# Call the embedding layer with input_ids
embeddings = emb_layer(input_ids)

# Convert to numpy array
embeddings_np = embeddings.numpy()

print(embeddings_np)


Q5. After creating a vocbulary based on the word-level (where word is defined by whitespace)tokenization of "chesterton-ball.txt".what would be the encoded version of the following"and picking out the few windows of the scattered hamlets"?


import nltk
import numpy as np
nltk.download('gutenberg')
text = nltk.corpus.gutenberg.raw('chesterton-ball.txt')
text1 = text.split()
word_list = sorted(set(text1))
vocab = {t:i for i,t in enumerate(word_list)}

sentence = 'and picking out the few windows of the scattered hamlets'
sentence = sentence.split(' ')
encoded_sentence = [vocab[t] for t in sentence]
encoded_sentence
